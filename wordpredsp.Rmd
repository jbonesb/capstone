---
title: "Next word prediction"
author: "Bogdanov Sergey"
date: '25.06.2019'
output: ioslides_presentation
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(stringr)
library(dplyr)
library(tidytext)
library(plyr)
library(data.table)
```
```{r, echo = FALSE, cache = TRUE}
load("stat.Rdata")
load("txt.RData")
```

## Task Description

This work describe language model development that must predict expected next word for user that will be typing some sentence.  

To train model [text corpus](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) composed form more than 4,2 mln sentences from blogs, news and tweets was used.

As prepare step for model development exploratory analysis was performed. It's described [here](http://rpubs.com/jbone/499816) in details.



## Predictive model

We use a [n-gram](https://en.wikipedia.org/wiki/N-gram) approach to produce language model.
Final model use trigrams and bigrams for next word prediction. 

To raise the accuracy of the model an interpolation method known as [Kneser-Nei Smoothing](https://www.google.com/search?rlz=1C1SQJL_ruRU829RU829&ei=mRwTXdbnBvGorgSPn6f4CQ&q=Kneser+Ney+smoothing+wikipedia) was used. This method helps smooth probability of n-grams direct counts by using information about how likely next word appears as a novel continuation.   

To minimize memory usage to store n-gram dictionaries  

- we use only 3 most likely outcomes for trigrams and bigrams,
- we code words with integers in that dictionaries.

You can find R code of model development and performance assessment on [github](https://github.com/jbonesb/capstone).


## Model performance

To assess model accuracy "out of sample" approach was used. We split initial sentence array on train and test datasets. Then we use train dataset to train the model and test dataset to accuracy assement.

Accuracy of final model presented in table below 
```{r stat}
report[,1:3]
```

And this model need around **330 Mb** of memory to store n-gram dictionaries.


## One picture is worth a thousand words

```{r, echo=FALSE}
shinyApp(

ui = fluidPage(
    
    # titlePanel("Next word prediction"),
    
    sidebarLayout(
        sidebarPanel(
            
            p("This is an application that predict next best word based on n-grams and use several backoff models and interpolation."),
            
            br(),
            p("Just start typing, then press whitespace and see how it works."),
            
            # br(),  
            # p("Model and application development description you can find", 
            #   a(href="http://rpubs.com/jbone/499816", "here"), "."),
            
            br()
        ),
        
        mainPanel(
            textAreaInput("strInput","Please, start typing", width = "150%", rows = 3),
            br(),

            actionButton("ab1", textOutput("tab1"), width = "30%"),
            actionButton("ab2", textOutput("tab2"), width = "30%"),
            actionButton("ab3", textOutput("tab3"), width = "30%"),
 
        br()
        )
    )
),

 server = function(input, output, session) {
    rv <- reactiveValues(pw1 = UD$word[1], pw2 = UD$word[2], pw3 = UD$word[3])

    observeEvent(input$ab1, {
        updateTextInput(session, "strInput", value =  paste(str_squish(input$strInput), rv$pw1, "" ) )
    })
    observeEvent(input$ab2, {
        updateTextInput(session, "strInput", value =  paste(str_squish(input$strInput), rv$pw2, "" ) )
    })
    observeEvent(input$ab3, {
        updateTextInput(session, "strInput", value =  paste(str_squish(input$strInput), rv$pw3, "" ) )
    })
    output$tab1 <- renderText({
        pwords()[1]
    })
    output$tab2 <- renderText({
        pwords()[2]
    })
    output$tab3 <- renderText({
        pwords()[3]
    })    
    
    pwords <- reactive({
        if (grepl(" $", input$strInput)) {
            t <- NULL
            words <- tibble(text= input$strInput) %>% unnest_tokens(word, text)

            if (nrow(words) == 1) {
 
                t <- UD[BDc[wc1 == UD[word == words$word[1],]$wc[1], ], on="wc==wc2"]
                    
                if (nrow(t) > 0) rv$pw1 = t$word[1]
                if (nrow(t) > 1) rv$pw2 = t$word[2]
                if (nrow(t) > 2) rv$pw3 = t$word[3]
            } 
            
            if (nrow(words) >= 2) {
                l = nrow(words)

                t <- UD[TDc[wc1 == UD[word == words$word[l-1],]$wc[1] &
                            wc2 == UD[word == words$word[l],]$wc[1], ], on="wc==wc3"]
                
                if (nrow(t) < 3) {
                    if (nrow(t) == 0){
                        t <- UD[BDc[wc1 == UD[word == words$word[l],]$wc[1], ], on="wc==wc2"]
                    }else{
                        t <- rbind.fill(t, UD[BDc[wc1 == UD[word == words$word[l],]$wc[1], ], on="wc==wc2"])
                    }
                }
                
                if (nrow(t) > 0) rv$pw1 = t$word[1]
                if (nrow(t) > 1) rv$pw2 = t$word[2]
                if (nrow(t) > 2) rv$pw3 = t$word[3] 
            } 
        }
        print(memory.size())
        gc()
        
        c(rv$pw1, rv$pw2, rv$pw3)
    })
  },

  options = list(height = 500)
)
# inputPanel(
#   selectInput("n_breaks", label = "Number of bins:",
#               choices = c(10, 20, 35, 50), selected = 20),
#   
#   sliderInput("bw_adjust", label = "Bandwidth adjustment:",
#               min = 0.2, max = 2, value = 1, step = 0.2)
# )
# 
# renderPlot({
#   hist(faithful$eruptions, probability = TRUE, breaks = as.numeric(input$n_breaks),
#        xlab = "Duration (minutes)", main = "Geyser eruption duration")
#   
#   dens <- density(faithful$eruptions, adjust = input$bw_adjust)
#   lines(dens, col = "blue")
# })
```




